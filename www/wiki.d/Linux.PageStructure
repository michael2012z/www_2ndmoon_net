version=pmwiki-2.2.6 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/534.7 (KHTML, like Gecko) Chrome/7.0.517.41 Safari/534.7
author=
charset=UTF-8
csum=
ctime=1288316272
host=114.80.140.34
name=Linux.PageStructure
rev=1
targets=
text=(:table class='markup horiz' align='center':)%0a(:cellnr  class='markup1':)%0a%0a/*%0a * Each physical page in the system has a struct page associated with%0a * it to keep track of whatever it is we are using the page for at the%0a * moment. Note that we have no way to track which tasks are using%0a * a page, though if it is a pagecache page, rmap structures can tell us%0a * who is mapping it.%0a */%0astruct page {%0a	unsigned long flags;		/* Atomic flags, some possibly%0a					 * updated asynchronously */%0a	atomic_t _count;		/* Usage count, see below. */%0a	union {%0a		atomic_t _mapcount;	/* Count of ptes mapped in mms,%0a					 * to show when page is mapped%0a					 * & limit reverse map searches.%0a					 */%0a		struct {		/* SLUB */%0a			u16 inuse;%0a			u16 objects;%0a		};%0a	};%0a	union {%0a	    struct {%0a		unsigned long private;		/* Mapping-private opaque data:%0a					 	 * usually used for buffer_heads%0a						 * if PagePrivate set; used for%0a						 * swp_entry_t if PageSwapCache;%0a						 * indicates order in the buddy%0a						 * system if PG_buddy is set.%0a						 */%0a		struct address_space *mapping;	/* If low bit clear, points to%0a						 * inode address_space, or NULL.%0a						 * If page mapped as anonymous%0a						 * memory, low bit is set, and%0a						 * it points to anon_vma object:%0a						 * see PAGE_MAPPING_ANON below.%0a						 */%0a	    };\\%0a#if USE_SPLIT_PTLOCKS%0a	    spinlock_t ptl;\\%0a#endif%0a	    struct kmem_cache *slab;	/* SLUB: Pointer to slab */%0a	    struct page *first_page;	/* Compound tail pages */%0a	};%0a	union {%0a		pgoff_t index;		/* Our offset within mapping. */%0a		void *freelist;		/* SLUB: freelist req. slab lock */%0a	};%0a	struct list_head lru;		/* Pageout list, eg. active_list%0a					 * protected by zone->lru_lock !%0a					 */%0a	/*%0a	 * On machines where all RAM is mapped into kernel address space,%0a	 * we can simply calculate the virtual address. On machines with%0a	 * highmem some memory is mapped into kernel virtual memory%0a	 * dynamically, so we need a place to store that address.%0a	 * Note that this field could be 16 bits on x86 ... ;)%0a	 *%0a	 * Architectures with slow multiplication can define%0a	 * WANT_PAGE_VIRTUAL in asm/page.h%0a	 */\\%0a#if defined(WANT_PAGE_VIRTUAL)%0a	void *virtual;			/* Kernel virtual address (NULL if%0a					   not kmapped, ie. highmem) */\\%0a#endif /* WANT_PAGE_VIRTUAL */%0a};%0a%0a%0a(:tableend:)
time=1288316272
author:1288316272=
diff:1288316272:1288316272:=1,72d0%0a%3c (:table class='markup horiz' align='center':)%0a%3c (:cellnr  class='markup1':)%0a%3c %0a%3c /*%0a%3c  * Each physical page in the system has a struct page associated with%0a%3c  * it to keep track of whatever it is we are using the page for at the%0a%3c  * moment. Note that we have no way to track which tasks are using%0a%3c  * a page, though if it is a pagecache page, rmap structures can tell us%0a%3c  * who is mapping it.%0a%3c  */%0a%3c struct page {%0a%3c 	unsigned long flags;		/* Atomic flags, some possibly%0a%3c 					 * updated asynchronously */%0a%3c 	atomic_t _count;		/* Usage count, see below. */%0a%3c 	union {%0a%3c 		atomic_t _mapcount;	/* Count of ptes mapped in mms,%0a%3c 					 * to show when page is mapped%0a%3c 					 * & limit reverse map searches.%0a%3c 					 */%0a%3c 		struct {		/* SLUB */%0a%3c 			u16 inuse;%0a%3c 			u16 objects;%0a%3c 		};%0a%3c 	};%0a%3c 	union {%0a%3c 	    struct {%0a%3c 		unsigned long private;		/* Mapping-private opaque data:%0a%3c 					 	 * usually used for buffer_heads%0a%3c 						 * if PagePrivate set; used for%0a%3c 						 * swp_entry_t if PageSwapCache;%0a%3c 						 * indicates order in the buddy%0a%3c 						 * system if PG_buddy is set.%0a%3c 						 */%0a%3c 		struct address_space *mapping;	/* If low bit clear, points to%0a%3c 						 * inode address_space, or NULL.%0a%3c 						 * If page mapped as anonymous%0a%3c 						 * memory, low bit is set, and%0a%3c 						 * it points to anon_vma object:%0a%3c 						 * see PAGE_MAPPING_ANON below.%0a%3c 						 */%0a%3c 	    };\\%0a%3c #if USE_SPLIT_PTLOCKS%0a%3c 	    spinlock_t ptl;\\%0a%3c #endif%0a%3c 	    struct kmem_cache *slab;	/* SLUB: Pointer to slab */%0a%3c 	    struct page *first_page;	/* Compound tail pages */%0a%3c 	};%0a%3c 	union {%0a%3c 		pgoff_t index;		/* Our offset within mapping. */%0a%3c 		void *freelist;		/* SLUB: freelist req. slab lock */%0a%3c 	};%0a%3c 	struct list_head lru;		/* Pageout list, eg. active_list%0a%3c 					 * protected by zone->lru_lock !%0a%3c 					 */%0a%3c 	/*%0a%3c 	 * On machines where all RAM is mapped into kernel address space,%0a%3c 	 * we can simply calculate the virtual address. On machines with%0a%3c 	 * highmem some memory is mapped into kernel virtual memory%0a%3c 	 * dynamically, so we need a place to store that address.%0a%3c 	 * Note that this field could be 16 bits on x86 ... ;)%0a%3c 	 *%0a%3c 	 * Architectures with slow multiplication can define%0a%3c 	 * WANT_PAGE_VIRTUAL in asm/page.h%0a%3c 	 */\\%0a%3c #if defined(WANT_PAGE_VIRTUAL)%0a%3c 	void *virtual;			/* Kernel virtual address (NULL if%0a%3c 					   not kmapped, ie. highmem) */\\%0a%3c #endif /* WANT_PAGE_VIRTUAL */%0a%3c };%0a%3c %0a%3c %0a%3c (:tableend:)%0a\ No newline at end of file%0a
host:1288316272=114.80.140.34
